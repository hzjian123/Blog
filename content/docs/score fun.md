+++
title = 'Score-based Generative Models'
date = 2023-12-25T21:24:45+08:00
draft = false
+++
# Introduction
Generative models can be classified into 2 categories:

**Implicit models** Probability distribution is implicit represented by a network and new samples are generated by passing a Gaussian noise as the input. One example is generative adversarial networks (GANs).

**Likelihood-based models** Directly learn probability distribution density by maximizing likelihood. Examples include variational auto-encoders (VAEs) and diffusion model.

We have derive the diffusion model from another page [Here](https://hzjian123.github.io/Blog/docs/diffusion/). We have introduced score function which is a objective of ELBO optimization. In this blog, we try to dive into the details of **score based model** and to what extent it is correlated with diffusion model.

# Score function, score-based models
Assume we use a data distribution $p(x)$ to generate a bunch of data $x_n$, our target is to learn this distribution. Here we define a probability density function (pdf) $f_\theta(x)$
$$p_\theta(x) = \frac{e^{-f_\theta(x)}}{Z_{\theta}}$$